\documentclass{article}
\usepackage{iclr2018_conference,times}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{subfigure}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\newcommand{\argmax}{\operatorname*{argmax}}
\newcommand{\argmin}{\operatorname*{argmin}}
\newcommand{\var}{\operatorname*{Var}}
\newcommand{\lstd}{\operatorname*{LSTD}}
\newcommand{\diag}{\operatorname*{diag}}
\newcommand{\sgn}{\operatorname*{sgn}}
\newcommand{\lin}{\operatorname*{lin}}
\newcommand{\supp}{\operatorname*{Supp}}
\newcommand{\todo}[2]{{\color{yellow}{#1 TODO: #2}}}
\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
%\input{publications.bbl}
\title{A survey on value-based deep reinforcement learning}
\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\author{Feng Xiaolong  \\
	\texttt{xlfeng886@163.com} \\
}
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\iclrfinalcopy
\begin{document}
\maketitle
\input{symbolsdef.tex}


.
\begin{abstract}
Reinforcement learning (RL) is developed to address the problem of how to make a sequential decision. The goal of the RL algorithm is to maximize the total reward when the agent interaction with the environment. RL is very successful in many traditional fields for decades. From another aspect of AI technology,  deep learning (DL) has become very popular in recent years and has extended the boundary of many tradition research filed. Combine with RL and DL is the fundamental idea for researchers recently. The first breakout comes from the DQN algorithm. We defined the new technology as deep reinforcement learning (DRL). DRL creates a new branch in both the RL field and the DL field. Since the DQN algorithm, there are many new algorithms that have been proposed, and the number of related papers has grown exponentially in recent years. In this paper, we focus on the value-based deep reinforcement learning method, which is one of the bases of the DRL algorithm sets. Many papers are derived from the original DQN algorithm.
\end{abstract}
%
\section{Introduction}
\label{sec:Introduction}
We can consider the essential question in computer science, how to build an efficient artificial intelligence (AI)? First, it must face the uncertain of the world and the complicated structure. Second, it should know how to make the sequence decision in the real world. Third, as \citep{sutton2018reinforcement} discussed the mechanism of how humans affected by the dopamine, the AI agent should also be derived by the reward and towards the maximum of the total reward during the learning progress. In recent years, DRL is the most popular method to approach the efficient AI system. DRL utilizes the DL method to deal with the unstructured of the world. 
In the past decades, the researchers hand-designed the relevant features when they deal with the specific problem. The traditional pipeline cost too many human resources but can not increase the performance too much. With the rapid development of DL technology and its widespread application in various fields, state-of-art in many areas has been continuously refreshed. The reinforcement learning field is also one of the fields that deeply affected by DL technology. As we know, RL is inspired by the behavioral psychology   \citep{sutton2018reinforcement}. It can handle the sequential decision-making problem. So, the combination of reinforcement learning and deep learning leads to a newborn deep reinforcement learning technology. The DRL technology fills us with visions of the future, and it has extended the boundaries of our cognition, allowing revolutionary changes in many applications.

We could find a lot of achievements brought by the DRL technology from \citep{lecun2015deep, schmidhuber2015deep, goodfellow2016deep}. For example, \citep{mnih2015human} utilized the DRL agent to learn the raw pixels of the Atari game and achieve human-level performance. \citep{silver2016mastering} can defeat human champions in the go game. \citep{silver2018general} use DRL to play chess and Shogi by the self-play method. \citep{vinyals2019alphastar} applied DRL in complicated strategy games and achieve a fantastic result. DRL also performs very well in real-world applications, such as robotics \citep{gandhi2017learning, pinto2017asymmetric}, self-driving cars \citep{pan2017virtual}. We also can see some very successful application cases in the finance field \citep{deng2016deep}. 

Here we need to have an overview of the DRL algorithm taxonomy. As we can see, DRL is a fast-developing field, and it is not easy to draw the taxonomy accurately. We can have some traditional perspectives for this, which ignore the most advanced area of DRL (such as meta-learning, transfer learning, MAML exploration, and some other fields).

The root branching points in a DRL algorithm is to consider the question of whether we can access to the environment model. If the state transitions and rewards can be predicted from the simulated model, we can consider it as the model-based DRL algorithm. If we have no idea about the transitions and rewards, we should consider it as the model-free DRL algorithm.

For a model-free DRL algorithms, we could have A2C algorithm, A3C algorithm\citep{mnih2016asynchronous}, PPO algorithm \citep{schulman2017proximal}, TRPO algorithm \citep{schulman2015trust}), Q-learning algorithm \citep{mnih2013playing}, C51 algorithm \citep{bellemare2017distributional}, QR-DQN algorithm \citep{dabney2018distributional}, ER algorithm \citep{andrychowicz2017hindsight}), TD3 algorithm \citep{fujimoto2018addressing}, SAC algorithm \citep{haarnoja2018soft}, etc.

For a model-based DRL algorithm, it should consider whether its model is given or the model is unknown. Alpha Zero \citep{silver2018general} is the application of the given model. If the model is unknown, we could have some algorithms such as I2A, MBMF, MBVE. 

In this article, we only survey the value-based deep reinforcement learning. 
%
\section{Problem formulation}
\label{sec:Problem formulation}
Reinforcement Learning \citep{sutton2018reinforcement} is to learn how to make sequential decision when the AI agent interact with the environments $\mathbb{E}$. We will formalize the $\mathbb{E}$ as a Markov Decision Processes (MDPs). it could be described as the tuple $({S},{A},{P},{R})$.  That means in each time step, the AI agent interact with the observed state $s_t \in {S}$, and chooses the action $a_t \in {A}$ . The action will lead to the reward $r_t \sim {R}$ and next state $s_{t+1} \sim {P}$.

The purpose of the learning process is to maximize the total reward when the AI agent interacts with the environments $\mathbb{E}$. In the infinite case, we also consider the discount factor $\gamma$ per time-step. The total discounted return  at time step $t$ should be  $R_t = \sum_{t'=t}^{T} \gamma^{t'-t} r_{t'}$, where $T$ is the time-step and we ignore the step before time step $t$ due to the causality law.

The optimal action-value function $Q^*(s,a)$  is  the maximum expected return on the condition below $s_t$ and $a_t$. We can define  it as $Q^*(s,a) = \max_{\pi} \mathbb{E}[{ R_t | s_t=s, a_t=a, \pi }]$, where $\pi$ is the policy. The $Q^*(s,a)$ function will follow the  Bellman equation and update the next $Q$ value iteratively as $Q_{i+1}(s,a) = \mathbb{E}\left[ r + \gamma \max_{a'} Q_i(s', a') | s, a \right]$. This is proved to converge to optimal $Q^ * $ in tabular case \citep{sutton2018reinforcement}. 
In the reinforcement learning community, this is typically a linear function approximator, but sometimes a non-linear function approximator is used instead, such as a neural network. When we use the neural network, we parameterize the $Q$-network according to the weight $\theta$ and train it by minimizing the mean square error (MSE) loss function. The parameter $\theta$ will change iteratively as below:

\begin{align}
	argmin _\theta  \ L_i\left(\theta_i\right) &= \mathbb{E}_{s,a \sim P(\cdot)}{\left( \mathbb{E}_{s' \sim {E}}[{r + \gamma \max_{a'} Q(s', a'; \theta_{i-1}) | s, a }] - Q \left(s,a ; \theta_i \right) \right)^2}
\end{align}


In the optimization progress, previous parameters  $\theta_{i-1}$ will be fixed.  The optimization method could be one of the most popular methods, such as ADAM, RMSprop, Momentum SGD, etc.
%

\section{Related work}
\label{sec:Related work}
For decades, scholars have made many outstanding contributions to the field of RL, and traditional RL algorithms are very mature.  We could find many papers that review the traditional RL, such as \citep{gosavi2009reinforcement, kaelbling1996reinforcement, sutton2018reinforcement, szepesvari2010algorithms}.

Benefit from the rapid development of DL technology, it is continuously refreshing the state-of-the-art in the RL field. There is also some survey cover the developments in the DRL field, such as \citep{arulkumaran2017brief}. There is also some paper survey on the DRL application in NLP filed, such as \citep{ranzato2015sequence} and \citep{bahdanau2016actor}.

%
\section{Traditional Value-based RL Algorithms}
\label{sec:Traditional Value-based RL Algorithms}
The value-based RL algorithms will define the policy $\pi$, by constructing a function of the state value or state -action value.  $Q$-learning algorithm \citep{watkins1992q} is the classical state-action value-based RL algorithm . We can find some traditional ways to improve the $Q$-learning by parameterized function approximator \citep{gordon1996stable}.

%Before we dive into the $Q$-learning, we should consider the Sarsa algorithm at first. We should consider Sarsa as an on-policy algorithm and consider $Q$-learning as an off-policy algorithm. 



%{\bf Sarsa Algorithm:}
%Unlike $Q$-learning algorithm to learn state value, the Sarsa algorithm is to learn the action-value function, $q_\pi (s,a)$. The reward will generated by $q_pi (s_t,a_t)$ and lead to new state-action pair $(s_{t+1},a_{t+1})$, so that we get the chain according to the state, action and reward. The detail of Sarsa algorithm can refer to \citep{sutton2018reinforcement}

%{\bf $Q$-learning Algorithm:}
{\bf $Q$-learning Algorithm:}
The simplest version of $Q$-learning is the tabular case, which looks up the $Q$ value table during the training process.  The  $Q$-learning algorithm applies the Bellman equation to update the Q-value function \citep{bellman1962dynamic}. The ieda is straightforward enough, but it works not very well in the high-dimensional state-action space. The parameterized value function  $Q(s, a; \theta)$ must be used to fit the value. The detail of the basic $Q$-learning algorithm can refer to \citep{sutton2018reinforcement}.

%{\bf Fitted $Q$-learning:}
{\bf Fitted $Q$-learning:}
As we could get a lot of experience, in the form of  $(s,a,r,s')$. In fitted $Q$-learning \citep{gordon1996stable}, it use an function approximator to update the  $Q(s, a; \theta_k )$ in each iteration, where  $\theta_k$ with respect to the parameter of  the function  in $k_{th}$ iteration. So we could obtain the target value as the equation below:

\begin{align}
y_k = r +\gamma \operatorname*{max}_{a' \in  A} Q(s',a';\theta_{k})
\end{align}


%{\bf Neural Fitted $Q$-learning (NFQ):}
{\bf Neural Fitted $Q$-learning (NFQ):}
The neural fitted $Q$-learning (NFQ) \citep{riedmiller2005neural}, The neural $Q$-network receives the state information as the input and output the probability of each action. The network is parameterized with a neural network $Q(s,a;\theta_k)$ and apply SGD to minimize the MSE. But the problem is that the target will be changed as the weights change and generate errors in the state-action space.  We cannot guarantee the convergence by using this algorithm \citep{baird1995residual, tsitsiklis1997analysis, gordon1996stable, riedmiller2005neural}. The other problem is the overestimate issue \citep{van2016deep}. The overestimate comes from the max operator when we estimate the value. The max operator will enlarge the noise during the training process.

%
\section{From DQN to Rainbow}
\label{sec:From DQN to Rainbow}

Here we specifically survey the deep $Q$-network (DQN) algorithm \citep{mnih2015human}. The DQN algorithm is the first algorithm that can achieve a superhuman level in the Arcade Learning Environment (ALE) \citep{bellemare2013arcade}. We will also survey various improvements that make up the Rainbow algorithm.

{\bf Deep $Q$-networks:}
There is some common sense as NFQ, and the DQN algorithm is so attractive due to the excellent performance in the variety of ATARI games. It directly learns the raw pixels from the video and keeps the same structure and parameters in all games.  This is unimaginable in previous algorithm implementations, which face the curve of dimension problem in high dimension space. 
To address the unstable and divergence problem, \citep{mnih2015human} have proposed two methods, experience replay, and target $Q$-network.

\begin{enumerate}[(a)]
	\item Experience replay mechanism: DQN initializes a replay buffer $\mathbf{D}$ with transitions $(s_t, a_t, r_t, s_{t+1})$ and randomly choose samples from the buffer to train the deep neural network. The simple idea is very efficient during the implementation.
	
	\item Fixed target $Q$-network: To address the shift of the $Q$-value problem, the target $Q$-network will periodically synchronize parameters with the main $Q$-networks. The performance of DQN seems to be more stable for this technology.
	
\end{enumerate}


{\bf Double DQN:}
To address the over-estimations  issue, \citep{van2016deep} proposes a solution to divide the $max$ operation into two parts, action selection part and action evaluation part.  Two networks evaluate and choose action values according to the loss function as below:

\begin{align}
Loss=\Big[ r_{t} + \gamma \hat {Q} \Big(s_{t+1}, \arg \underset{a_{t+1}}{\max} {Q} \big(s_{t+1}, a_{t+1};{\theta} \big); {\theta}' \Big) - {Q}(s_t,a_t;{\theta}) \Big]^2
\end{align}


The evaluation network is parameterized by ${\theta}'$, and apply $\epsilon-greedy$ policy. The evaluate network will periodically be synchronized with the estimate network.

{\bf Dueling DQN:}
We can decompose the Q-value function into value function and advantage function. So, the dueling DQN used a specialized dueling network architecture to implement this simple idea. \citep{wang2015dueling} proposes to change the single output network to a double output network. They share the same encoder. The formulation can be considered as below:

\begin{align}
{Q} (s,a;{\theta_{1}},{\theta_{2}}) =  {V}(s;{\theta_{1}}) + \Big({A} (s,a;{\theta_{1}}) - \frac{\sum_{a'} {A} (s,a';{\theta_{1}})}{|{A}|}		\Big) ,
\end{align}

where $\theta_1$ is the parameter of advantage function structure, and $\theta_1$ is the parameter of the value function structure. 


{\bf Prioritized Replay:}
In the DQN algorithm, it proposes to use replay buffer technology and the algorithm uniformly sample from the buffer to train the neural network. \citep{schaul2015prioritized} have a one more step ieda for this technology, the proposed prioritized replay buffer. They set the priority of the experience in the buffer according to the defined rules, and then sample the experience by the priority. The high priority experience is worse to learn. Priority rules are usually set using TD error, but the choice of this method could be defined according to the particular problem. So, it is not easy when we could not know what experience is essential and what is not.

{\bf Asynchronous Multi-step DQN:}
The previous DQN algorithm needs to consider using off-policy algorithms to improve efficiency, and it uses the replay buffer, which occupied much memory and computation resources. \citep{mnih2016asynchronous} proposes an idea to use many agents to train the deep neural network. It applies the asynchronous gradient descent method to train the multiple agents. Each subprocess maintains its own environment, and all the accumulated gradient will be applied to the center.
The n-step methods \citep{sutton2018reinforcement} will be used to update the reward function, and it will be used to trade off the bias and variance problem.

{\bf Distributional DQN:}
Normally, the Bellman equation is used to calculate the expected reward value. But \citep{bellemare2017distributional} introduces a different solution that uses the distributional perspective to update the Q-value function. Their motivation is to address the issue caused by the uncertainty of the system, which has multimodal distribution in action space.

In their work, they let  ${Z}(s,a)$ be the return obtained by the current policy. ${Z}$ is not a scalar but a distribution. Then we derived the distributional version of Bellman equation as follows: ${Z}(s,a) = r + \gamma {Z}(s',a')$. 

This advanced proposal has been well used in practice \citep{bellemare2017distributional}, \citep{dabney2018distributional},  \citep{rowland2018analysis}. This method could lead to risk-aware behavior \citep{morimura2010nonparametric} and leads to more robust learning in practice. The reason could be that the distributional perspective maybe provides more training signals than the previous method.

{\bf Noisy DQN:}
\citep{fortunato2017noisy} proposal the Noisy Net. The noisy network could have the same structure as before, but its bias and weights could be perturbed iteratively. The motivations are to solve the limitations of exploring using $\epsilon$-greedy policies. It usually will combine the deterministic and Gaussian noisy stream.
In the implementation status, the randomized action-value function replaces the $\epsilon$-greedy policy. Then, the MLP layers will be parameterized by the noisy network. 

{\bf Rainbow DQN:}
\citep{matteo2017rainbow} proposes a solution that integrates all advantages of the seven methods, include DQN, double DQN, prioritized DDQN, dueling DDQN, multi-step, distributed DQN, and noisy DQN. Due to the ensemble of the seven methods, it called the Rainbow DQN.  The combinations lead to a mighty learning algorithm, which is more stable, robust, and efficiency than any single method.
%
\section{The Other value-based DRL alogrithms}
\label{sec:The Other value-based DRL alogrithms}
{\bf Deep Q-learning from Demonstrations(DQfD):}
The motivation of the DQfD algorithm \citep{sendonaris2017learning} is to address the problem that a large amount of data is required during the training of the DQN algorithm. We have to face the fact that we cannot get enough data from the real application. This issue also severely restricts the boundary of deep reinforcement learning. To take full advantage of the non-real-time data, which is generated from the demonstration of people or offline machines, The DQfD algorithm is proposed to accelerate the learning process by only a small amount of demonstration data. The core ieda of the DQfD algorithm is:
\begin{enumerate}[(a)]
	\item Collect the demonstration data and add it to the replay buffer.
	\item Train the network with the demonstration data before the interaction process.
	\item It combines 1-step TD loss, L2 regularization loss, supervised loss, and n-step TD loss.
\end{enumerate}

{\bf Sym DQN Algorithm:}
\citep{mahajan2017symmetry} proposes the Sym DQN algorithm to ensure the improvement of sampling efficiency through symmetries in complex DRL tasks. Symmetries in MDP can be defined by the concept of MDP homomorphisms. The definition can refer to the original article. Sym DQN contains two components, one is the similarity detecting procedure, and the other is the similarity incorporating. The previous component will calculate the similarity estimates the value, and the later component will help the learning of symmetric policies. The network training process can utilize the symmetries of the state-action pair and learn the state-action pair representation on the top of the network to output the same representation for the state-action pair. 

{\bf Guided DQN Algorithm:}
\citep{taitler2017learning} proposes the Guided DQN algorithm to improve the performance of the original DQN algorithm. There is two basis improvement. 
The first ieda is to integrate with the prior knowledge during the learning process, for example, in the task of how to play StarCraft, the rules of the StarCraft game can guide the agent to choose the better action. 
The second improvement is to adjust the update interval time for the parameter of the target network. This improvement could avoid decreased performance when implementing the original DQN algorithm. Guided DQN chooses a short update interval at the beginning and chooses the larger update interval as the experience replay buffer increase. This method could make the reward more stable, and the selected policy could keep around the optimal policy.

{\bf LS-DQN Algorithm:}
\citep{levine2017shallow} proposes the least squares deep Q-network(LS-DQN) algorithm to combine the DRL with the linear least square method. On the one hand, DRL can learn rich feature representations. On the other hand, using the least square method can make the learning process more stable. In their work, the last hidden layer should be repeatedly retrained base one the fitted Q iteration algorithm. To address the over-fitting issue, they use the bayesian regularization term for the least squares update. The experiment result shown the better performance of the LS-DQN algorithm than the DQN algorithm and DDQN algorithm in Atari games.

{\bf BBQN Algorithm:}
\citep{lipton2016efficient} proposes the Bayes-by-Backprop Q network to address the problem of worse performance when applying the $\epsilon$-greedy exploration method in the high dimensional action space. In the process of action selection, the BBQN algorithm quantified the uncertainty of the Q function estimation and used it to guide the exploration process. In the DQN algorithm, the Q function will be represented by the neural network with parameter $w$. But in the BBQN algorithm, parameter $w$ was considered as the random variable, and it will calculate the posterior probability distribution $q(w|\theta)$ of the parameter $w$.  The posterior probability distribution $q$ is the multivariable Gaussian distribution with diagonal covariance, that's mean the parameter $w$ is sampled from the $N(\mu,\delta_i^2)$. The $\delta_i$ will be parameterized by $\delta_i=log(1+exp(\rho_i))$ to make sure all the $\delta_i$ to be positive. Given the posterior probability distribution $q$, the Thompson sampling method is used for the exploration. The BBQN algorithm, which uses the Bayes-by-Backprop neural network to approximate the Q function, is shown to be faster than the traditional DQN algorithm.

{\bf DQN with penalty signal:}
\citep{leibfried2017information} introduces the penalty signal for the DQN algorithm to address the overestimation issue. Their work is inspired by the concept of information theory, and the penalty signal could inject the randomness for reward function.

{\bf DOL Algorithm:}
\citep{mossalam2016multi} proposes Deep Optimistic Linear Support Learning (DOL) to handle the multi-objective decision issue. The DOL algorithm combines the optimistic linear support(OLS) \citep{roijers2015computing} framework with DQN. It obtains the vector $w$, which decides the importance between the multi-objective. Then, it minimize the inner product between the $Q$-value vector $Q(s,a;\theta)$ and $w$ by optimize the parameter $\theta$. To match the requirement of the OLS framework, they modify the output of the DQN architecture to %n% output $Q$-values instead of the single output. In other words, the output of the neural network is the Q value of each action corresponding to each objective.
As the learning process progresses, each goal will become more and more similar, that is, their corresponding optimal $Q$-values will become more and more similar. Because DQN can be used to extract the features, so the network parameters of the previous target network can be reused to speed up the learning process of the next target network. According to how the network parameters are used, it is divided into two algorithms, Deep OLS Learning with Full Reuse(DOL-FR) algorithm and Deep OLS Learning with Partial Reuse(DOL-FR). The experiment result shows the DOL algorithm is much more accurately than the DQN algorithm.

{\bf Averaged-DQN Algorithm:}
\citep{anschel2017averaged} proposes the averaged-DQN algorithm to address the instability and variability issue for the original DQN algorithm. This algorithm averages the previous Q value estimates to make the training process more stable, and improves performance by reducing target approximation error(TAE). The averaged Q value have the below formulation:

\begin{align}
Q_{t}^{average}=\frac{1}{N}\sum_{n=1}^N{Q(s,a;\theta_{t-n}}
\end{align}

Averaged-DQN algorithm will replace the target as:

\begin{align}
y_t= r +\gamma \operatorname*{max}_{a' \in  A} Q^{average}(s',a';\theta_{t})
\end{align}

This algorithm is a very semple extension for the original DQN algorithm, but the experiment has shown to be more stable than DQN algorithm.

{\bf DRQN Algorithm:}
\citep{hausknecht2015deep} proposes the Deep Recurrent Q-Network(DRQN) algorithm. This algorithm combines the LSTM network with DQN algorithm, and replace the MLP layers with the LSTM network. The combination could address the limited memory issue and the problem that the DQN relies on the complete game screen during the Atari task.

{\bf ADRQN Algorithm:}
\citep{zhu2017improving} proposes the Action-specific Deep Recurrent Q-Network (ADRQN) algorithm to improve the performance in partially observable Markov decision process(POMDP). They encode the history action and observation to feed the $Q$-network. ADRQN algorithm change the original transistion $(s_t,a_t,r_t,s_{t+1})$ to $(\{a_{t-1},o_t\},a_t,r_t,s_{t+1})$, and the improvement lead more efficiency in partially observable domains.

{\bf Bootstrapped DQN Algorithm:}
\citep{osband2016deep} proposes the bootstrapped DQN algorithm to address the low efficiency of the $\epsilon$-greedy strategy. This algorithm combines deep exploration with DNN. It can effectively estimate the uncertainty of the DNN, and can achieve faster exponential speed learning than the convergence speed of the learning process of any random dithering strategy. This algorithm can be naturally extended to massively parallel systems.

{\bf DQN with log prior augmentation:}
\citep{jaques2017sequence} proposes the Deep $Q$-learning with log prior augmentation algorithm to improve the structure and quality of sequence, which is generated by the RNN network. This algorithm also retains the information obtained from the data and the diversity of the samples. First, it will pre-train the LSTM network by maximum likelihood estimation. Second, it assigns initial weights to three neural networks, $Q$-network, target network, and reward RNN network. Among them, the connection weight matrix and bias of the reward RNN network remain unchanged during the training process, and the prior strategy $\pi(a_t|s_t)$ is output at the same time.
To apply the RL method to the sequence generation task, the state at time $t$ is defined as $s_t=\{a_1,a_2,\cdot \cdot \cdot, a_t\}$. The total reward is defined as the bellow equation with the constant $c$ to trade off the importance of the reward.

\begin{align}
r(s,a)=log\pi(a|s)+r_T(s,a)/c
\end{align}

Apply the reward to the DQN algorithm, they could get the target function as below:

\begin{align}
L(\theta)=\mathbf{E}_\beta[(r(s,a)+\gamma \operatorname*{max}_{a'}Q(s',a';\theta^-)-Q(s,a;\theta))^2]
\end{align}




{\bf Gorila DQN Algorithm:}
\citep{nair2015massively} apply the general reinforcement learning architecture to the DQN algorithm. The Gorila DQN has massively distributed architecture for DRL. The architecture includes four components: parallel actors, parallel learners, distributed NN to represent the value, and the distributed storage of experience.

{\bf DARQN Algorithm:}
\citep{sorokin2015deep} proposes the deep attention recurrent Q-network (DARQN) algorithm to introduce the attention mechanism into DQN network, include the soft attention mechanism and the hard attention mechanism. The DARQN algorithm could select and focus on the ability to process smaller input image information regions, so it can also reduce the number of parameters in the deep neural network and the computational cost for training and testing.

{\bf DQN with Intrinsic Fear:}
\citep{lipton2016combating} introduces intrinsic fear into the DQN to avoid the catastrophic states during the training. The key to the implementation is to train a danger model. It is a binary classification neural network, which has the same architecture with the DQN network except for the output layer. The danger model could predict the probability of the imminent catastrophe. The new algorithm changes the optimization target to the following equation.

\begin{align}
y^{IntrinsicFear}=r+\operatorname*{max}_{a'}Q(s',a';\theta_Q)-\lambda\cdot d(s';\theta_d)
\end{align}

The $(s';\theta_d)$ represents the danger model, and $\lambda$ is the fear factor, which trades off the importance of the intrinsic fear.
%
\section{Conclusion}
\label{sec:Conclusion}
In the view of the theoretical significance and practical application value of DRL technology, this article surveys the value-based deep reinforcement learning technology. We mainly dived into the DQN algorithm and the improvement according to it. Many related improved algorithms are designed to solve a specific problem. According to the different emphasis on the improvement of the DQN algorithm, it can be divided into the improvement of training algorithm, improvement of neural network architecture, improvement of introducing new learning mechanism, and improvement based on newly proposed RL algorithm. We have only reviewed some of the improved algorithms, and there are still many other excellent algorithms for further work.
%

\bibliography{ValueBasedDRLSurvey}
%\input{test.bbl}
\bibliographystyle{iclr2018_conference}
\newpage
\appendix
 
%
\end{document}

